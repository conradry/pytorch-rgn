{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGN Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper is at: https://www.biorxiv.org/content/biorxiv/early/2018/02/14/265231.full.pdf\n",
    "\n",
    "There are a lot of details missing, but the architecture is fairly simple. Feed sequences into an bi-LSTM and predict a set of three torsion angles. Pass the three predictions along with the current atoms for each residue into a \"geometric unit\", add each residue sequentially and deform the \"nascent structure\" appropriately. The last step is to calculate the loss, distance-based root mean square deviation (dRMSD), which accounts for global and local structural details and importantly does not require a specific orientation of the predicted structure since it only considers distance between pairs of atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as ip\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter as cs\n",
    "import sys\n",
    "import Bio.PDB as bio\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim\n",
    "import pdb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from data import ProteinNet, sequence_collate\n",
    "from model import *\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.curdir + '/data/'\n",
    "pdb_path = os.curdir + '/data/pdb/structures/pdb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data from github and run the proteinnet notebook first\n",
    "trn_dataset = ProteinNet(data_path+'train30.bc')\n",
    "val_dataset = ProteinNet(data_path+'validation.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = DataLoader(trn_dataset, batch_size=32, shuffle=True, collate_fn=sequence_collate)\n",
    "val_data = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=sequence_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, torch.Size([622, 32, 41]), torch.Size([1866, 32, 3]))\n",
      "(1, torch.Size([676, 32, 41]), torch.Size([2028, 32, 3]))\n",
      "(2, torch.Size([695, 32, 41]), torch.Size([2085, 32, 3]))\n",
      "(3, torch.Size([731, 32, 41]), torch.Size([2193, 32, 3]))\n"
     ]
    }
   ],
   "source": [
    "#there should be exactly 3 coordinates for each residue\n",
    "for i_batch, sample_batched in enumerate(trn_data):\n",
    "    vec = sample_batched['sequence']\n",
    "    print(i_batch, sample_batched['sequence'].size(),\n",
    "         sample_batched['coords'].size())\n",
    "    if i_batch == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, linear_units=20, input_size=42):\n",
    "        super(RGN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.linear_units = linear_units\n",
    "        self.grads = {}\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, num_layers, bidirectional=True)\n",
    "        \n",
    "        #initialize alphabet to random values between -pi and pi\n",
    "        u = torch.distributions.Uniform(-3.14, 3.14)\n",
    "        self.alphabet = nn.Parameter(u.rsample(torch.Size([linear_units, 3])))\n",
    "        self.linear = nn.Linear(hidden_size*2, linear_units)\n",
    "        \n",
    "        #set coordinates for first 3 atoms to identity matrix\n",
    "        self.A = torch.tensor([0., 0., 1.])\n",
    "        self.B = torch.tensor([0., 1., 0.])\n",
    "        self.C = torch.tensor([1., 0., 0.])\n",
    "\n",
    "        #bond length vectors C-N, N-CA, CA-C\n",
    "        self.avg_bond_lens = torch.tensor([1.329, 1.459, 1.525])\n",
    "        #bond angle vector, in radians, CA-C-N, C-N-CA, N-CA-C\n",
    "        self.avg_bond_angles = torch.tensor([2.034, 2.119, 1.937])\n",
    "\n",
    "    \n",
    "    def forward(self, sequences, lengths):\n",
    "        max_len = sequences.size(0)\n",
    "        batch_sz = sequences.size(1)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long, requires_grad=False)\n",
    "        order = [x for x,y in sorted(enumerate(lengths), key=lambda x: x[1], reverse=True)]\n",
    "        conv = zip(range(batch_sz), order) #for unorder after LSTM\n",
    "        \n",
    "        #add absolute position of residue to the input vector\n",
    "        abs_pos = torch.tensor(range(max_len), dtype=torch.float32).unsqueeze(1)\n",
    "        abs_pos = (abs_pos * torch.ones((1, batch_sz))).unsqueeze(2) #broadcasting\n",
    "        \n",
    "        h0 = Variable(torch.zeros((self.num_layers*2, batch_sz, self.hidden_size)))\n",
    "        c0 = Variable(torch.zeros((self.num_layers*2, batch_sz, self.hidden_size)))\n",
    "        \n",
    "        #input needs to be float32 and require grad\n",
    "        sequences = torch.tensor(sequences, dtype=torch.float32, requires_grad=True)\n",
    "        pad_seq = torch.cat([sequences, abs_pos], 2)\n",
    "    \n",
    "        packed = pack_padded_sequence(pad_seq[:, order], lengths[order], batch_first=False)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(packed, (h0,c0))\n",
    "        unpacked, _ = pad_packed_sequence(lstm_out, batch_first=False, padding_value=0.0)\n",
    "        \n",
    "        #reorder back to original to match target\n",
    "        reorder = [x for x,y in sorted(conv, key=lambda x: x[1], reverse=False)]\n",
    "        unpacked = unpacked[:, reorder]\n",
    "\n",
    "        #for example, see https://bit.ly/2lXJC4m\n",
    "        softmax_out = F.softmax(self.linear(unpacked), dim=2)\n",
    "        sine = torch.matmul(softmax_out, torch.sin(self.alphabet))\n",
    "        cosine = torch.matmul(softmax_out, torch.cos(self.alphabet))\n",
    "        out = torch.atan2(sine, cosine)\n",
    "        \n",
    "        #create as many copies of first 3 coords as there are samples in the batch\n",
    "        broadcast = torch.ones((batch_sz, 3))\n",
    "        pred_coords = torch.stack([self.A*broadcast, self.B*broadcast, self.C*broadcast])\n",
    "        \n",
    "        for ix, triplet in enumerate(out[1:]):\n",
    "            pred_coords = geometric_unit(pred_coords, triplet, \n",
    "                                         self.avg_bond_angles, \n",
    "                                         self.avg_bond_lens)\n",
    "        #pred_coords.register_hook(self.save_grad('pc'))\n",
    "        \n",
    "            \n",
    "        #pdb.set_trace()\n",
    "        return pred_coords\n",
    "    \n",
    "    def save_grad(self, name):\n",
    "        def hook(grad): self.grads[name] = grad\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, torch.Size([676, 32, 41]), torch.Size([2028, 32, 3]), torch.Size([2028, 32, 3]))\n",
      "(1, torch.Size([731, 32, 41]), torch.Size([2193, 32, 3]), torch.Size([2193, 32, 3]))\n",
      "(2, torch.Size([414, 32, 41]), torch.Size([1242, 32, 3]), torch.Size([1242, 32, 3]))\n",
      "(3, torch.Size([563, 32, 41]), torch.Size([1689, 32, 3]), torch.Size([1689, 32, 3]))\n"
     ]
    }
   ],
   "source": [
    "#make sure output size and target sizes are the same\n",
    "for i_batch, sampled_batch in enumerate(trn_data):\n",
    "    inp_seq = sampled_batch['sequence']\n",
    "    inp_lens = sampled_batch['length']\n",
    "    rgn = RGN(20, 1, 20)\n",
    "    out = rgn(inp_seq, inp_lens)\n",
    "    print(i_batch, inp_seq.size(), sampled_batch['coords'].size(), out.size())\n",
    "    \n",
    "    if i_batch == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the author:\n",
    "\n",
    "The biggest issue with training these models, especially if you’re using ProteinNet with the full length proteins, is that they’re very seed sensitive, extremely so. Often you won’t find a good seed for hundreds of trials. What I do to get around this problem is set up a milestone scheme where if the validation error hasn’t dropped below a certain threshold by a certain iteration, I kill the model and start over. For example for ProteinNet12, here are my milestones using validation dRMSD (angstroms) (showing iterations not epochs):\n",
    "\n",
    " \n",
    "<ul>\n",
    "    <li>1k: 13.5</li>\n",
    "    <li>5k: 12.6</li>\n",
    "    <li>20k: 12.2</li>\n",
    "    <li>50k: 11.4</li>\n",
    "    <li>100k: 10.6</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters are directly from the paper's author\n",
    "rgn = RGN(800, 2, linear_units=60)\n",
    "#rgn.load_state_dict(torch.load(data_path+'models/rgn.pt')) #load pretrained model\n",
    "optimizer = torch.optim.Adam(rgn.parameters(), lr=1e-3)\n",
    "drmsd = dRMSD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(30):\n",
    "    last_batch = len(trn_data) - 1\n",
    "    for i, data in tqdm(enumerate(trn_data)):\n",
    "        names = data['name']\n",
    "        coords = data['coords']\n",
    "        mask = data['mask']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rgn(data['sequence'], data['length'])\n",
    "\n",
    "        loss = drmsd(outputs, coords, mask)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(rgn.parameters(), max_norm=50)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i != 0) and (i % last_batch == 0):\n",
    "            print('Epoch {}, Train Loss {}'.format(epoch, running_loss/i))\n",
    "            running_loss = 0.0\n",
    "            break\n",
    "            \n",
    "    last_batch = len(val_data) - 1\n",
    "    for i, data in tqdm(enumerate(val_data)):\n",
    "        names = data['name']\n",
    "        coords = data['coords']\n",
    "        mask = data['mask']\n",
    "        \n",
    "        outputs = rgn(data['sequence'], data['length'])\n",
    "        loss = drmsd(outputs, coords, mask)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i != 0) and (i % last_batch == 0):\n",
    "            print('Epoch {}, Val Loss {}'.format(epoch, running_loss/i))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rgn.state_dict(), data_path+'models/rgn.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
